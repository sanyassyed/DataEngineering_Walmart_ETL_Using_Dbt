{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://weclouddata.s3.amazonaws.com/images/logos/wcd_logo_new_2.png\" align=\"center\" style=\"zoom:50%;\"> \n",
    "</center>\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    <font size='6'>\n",
    "        dbt(Data Build Tool) Tutorial\n",
    "    </font>\n",
    "</p>\n",
    "\n",
    "<br>\n",
    "    <center align=\"left\">\n",
    "        <font size='4'>\n",
    "            Developed by: \n",
    "        </font>\n",
    "        <font size='4' color='#33AAFBD'>\n",
    "            WeCloudData\n",
    "        </font>\n",
    "    </center>\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing DBT on an Ubuntu Virtual Machine: Step-by-Step Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Ensure EC2 Instance is running\n",
    "\n",
    "   - Confirm that your EC2 instance is operational and running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Set Up Virtual Studio Code\n",
    "\n",
    "   - Install Virtual Studio Code.\n",
    "   - Add the \"Remote - SSH\" Extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Configure SSH Config File\n",
    "\n",
    "   - Create or edit **~/.ssh/config** on your local machine.\n",
    "   - Include an entry with the instance alias, the EC2 instance‚Äôs public IP address, and the file path to your .pem key file.\n",
    "\n",
    "      ```bash\n",
    "         Host your-instance-alias\n",
    "            HostName your-ec2-public-ip\n",
    "            User ubuntu\n",
    "            IdentityFile /path/to/your/keyfile.pem\n",
    "      ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Connect via Visual Studio Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Check Python Installation (Prerequisite)\n",
    "\n",
    "   - Verify if you have Python or Python3 installed by checking the current version.\n",
    "\n",
    "      ```bash\n",
    "         python --version\n",
    "         # or \n",
    "         python3 --version\n",
    "      ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Update python and pip libs (Optional)\n",
    "\n",
    "```bash\n",
    "    pip --version\n",
    "    python3 -m pip install --upgrade pip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.Install DBT\n",
    "\n",
    "   - Install DBT using the pip command. \n",
    "\n",
    "      üìù Installing dbt-snowflake will also install dbt-core and any other dependencies.\n",
    "\n",
    "      ```bash\n",
    "         pip install dbt-snowflake\n",
    "      ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.Verify the Installation\n",
    "\n",
    "   - Run the command to verify if DBT is installed correctly.\n",
    "\n",
    "      ```bash\n",
    "         dbt --version\n",
    "      ```\n",
    "\n",
    "      If you have followed all the steps correctly, you should see the installed DBT version printed on the terminal.\n",
    "\n",
    "      <img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/1_check_dbt_version.png\" alt=\"1_check_dbt_version\" style=\"zoom:40%;\"/>\n",
    "\n",
    "*Congratulations! You have successfully installed DBT on the Ubuntu vitrual machine. You are now ready to start using DBT for transforming and modeling your data.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiating a DBT Project\n",
    "\n",
    "Use a descriptive project name that reflects the purpose of the project. **Project name can only contain letters, digits, and uncerscores.**  Avoid using spaces, special characters, or starting the project name with a number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Initiate a dbt project named \"demo\".\n",
    "\n",
    "```bash\n",
    "   dbt init demo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Choose a suitable adapter to connect to your data source, such as BigQuery, Snowflake, Redshift, or Postgres.\n",
    "\n",
    "   - For demo purposes, we will choose snowflake. It will create a basic dbt project template along with configuration file.\n",
    "\n",
    "\\\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/2_initiate_a_dbt_project.png\" alt=\"2_initiate_a_dbt_project\" style=\"zoom:50%;\" />\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced: Customizing a Profile Directory (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.After you initiate your dbt project, .dbt directory will be created at your $HOME path by default. The profiles.yml will contain project-specific connectivity information of the data warehouse.\n",
    "\n",
    "```bash\n",
    "   # display the contents of the profiles.yml file located in the .dbt directory of the user's home folder\n",
    "   cat ~/.dbt/profiles.yml\n",
    "\n",
    "   # open the profiles.yml file located in the .dbt directory of the user's home folder for editing, using the vi, vim, or nano text editors, respectively.\n",
    "   vi ~/.dbt/profiles.yml\n",
    "   # or\n",
    "   vim ~/.dbt/profiles.yml\n",
    "   # or\n",
    "   nano ~/.dbt/profiles.yml\n",
    "\n",
    "   \n",
    "   # changes the current working directory to the demo folder within the current location in the terminal.\n",
    "   cd demo\n",
    "\n",
    "   # checks the dbt project for issues by validating the environment, connections, and project files\n",
    "   dbt debug\n",
    "   # or\n",
    "   # additionally displays the directory where the dbt configuration is stored\n",
    "   dbt debug --config-dir\n",
    "```\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/3_1_dbt_debug.png\" alt=\"3_1_dbt_debug\" style=\"zoom:40%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.You may want to have your profiles.yml file stored in a **different directory** than *~/.dbt/*. For example, if you are using environment variables to load your credentials, you might choose to include this file in the root directory of your dbt project.\n",
    "\n",
    "```bash\n",
    "   # displays the full pathname of the current working directory in the terminal\n",
    "   pwd\n",
    "\n",
    "   # move profiles.yml to path/to/directory\n",
    "   mv ~/.dbt/profiles.yml path/to/directory \n",
    "\n",
    "   # replace path/to/directory with your pwd\n",
    "   export DBT_PROFILES_DIR=path/to/directory\n",
    "\n",
    "   # changes the current working directory to the demo folder within the current location in the terminal.\n",
    "   cd demo\n",
    "\n",
    "   # checks the dbt project for issues by validating the environment, connections, and project files\n",
    "   dbt debug\n",
    "```\n",
    "\n",
    "üìù the file always needs to be called `profiles.yml`, regardless of which directory it is in.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/3_2_dbt_debug_advanced.png\" alt=\"3_2_dbt_debug_advanced\" style=\"zoom:40%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced: Custom Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does DBT Generate a Model's Schema Name?\n",
    "\n",
    "DBT uses a default macro called `generate_schema_name` to determine the name of the schema that a model should be built in.\n",
    "\n",
    "The following code represents the default macro's logic:\n",
    "\n",
    "```sql\n",
    "    {% macro generate_schema_name(custom_schema_name, node) -%}\n",
    "\n",
    "        {%- set default_schema = target.schema -%}\n",
    "        {%- if custom_schema_name is none -%}\n",
    "\n",
    "            {{ default_schema }}\n",
    "\n",
    "        {%- else -%}\n",
    "\n",
    "            {{ default_schema }}_{{ custom_schema_name | trim }}\n",
    "\n",
    "        {%- endif -%}\n",
    "\n",
    "    {%- endmacro %}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Schema Configuration\n",
    "\n",
    "You can customize schema name generation in dbt depending on your needs, such as creating a custom macro named get_custom_schema.sql\n",
    "\n",
    "üìù The .sql file name doesn't need to align with the macro's name.\n",
    "\n",
    "```sql\n",
    "    {% macro generate_schema_name(custom_schema_name, node) -%}\n",
    "\n",
    "        {%- set default_schema = target.schema -%}\n",
    "        {%- if custom_schema_name is none -%}\n",
    "\n",
    "            {{ default_schema }}\n",
    "\n",
    "        {%- else -%}\n",
    "\n",
    "            {{ custom_schema_name | trim }}\n",
    "\n",
    "        {%- endif -%}\n",
    "\n",
    "    {%- endmacro %}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Use Custom Schemas?\n",
    "\n",
    "Use the schema configuration key to specify a custom schema for a model. As with any configuration, you can either:\n",
    "\n",
    "- apply this configuration to a specific model using a config block within that model, or\n",
    "\n",
    "\t<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/4_1_how_to_use_custom_schemas.png\" alt=\"4_1_how_to_use_custom_schemas\" style=\"zoom:50%;\" />\n",
    "\n",
    "- apply the configuration to a specific model by specifying it in the properties.yml file, or\n",
    "\n",
    "\t<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/4_2_how_to_use_custom_schemas.png\" alt=\"4_2_how_to_use_custom_schemas\" style=\"zoom:50%;\" />\n",
    "\t\n",
    "- apply it to a subdirectory of models by specifying it in your dbt_project.yml file\n",
    "\n",
    "\t<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/4_3_how_to_use_custom_schemas_advanced.png\" alt=\"4_3_how_to_use_custom_schemas_advanced\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlining Sales Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Sources to DAG\n",
    "\n",
    "Sources make it possible to name and describe the data loaded into your warehouse by your Extract and Load tools. By declaring these tables as sources in dbt, you can then \n",
    "\n",
    "- select from source tables in your models using the `{{ source() }}` function\n",
    "- helping define the lineage of your data test your assumptions about your source data\n",
    "- calculate the freshness of your source data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring a source\n",
    "\n",
    "Sources are defined in `.yml` files nested under a `sources:` key.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/4_4_sources_yml.png\" alt=\"4_4_sources_yml\" style=\"zoom:50%;\" />\n",
    "\n",
    "üìù By default, `schema` will be the same as `name`. Add `schema` only if you want to use a source name that differs from the existing schema.\n",
    "\n",
    "If you're not already familiar with these files, be sure to check out the [documentation on schema.yml files](https://docs.getdbt.com/reference/configs-and-properties) before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product table\n",
    "\n",
    "Quick peek of the initial *raw.product* table:\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/5_1_initial_product.png\" alt=\"5_1_initial_product\" style=\"zoom:100%;\" />\n",
    "\n",
    "Quick peek of the updated *raw.product* table:\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/5_1_updated_product.png\" alt=\"5_1_updated_product\" style=\"zoom:200%;\" />\n",
    "\n",
    "The objective is to employ both the `merge incremental materialization strategy` and `dbt snapshot technique` to optimize the process of inserting and updating new records into the product table. Additionally, we will analyze and compare the differences between these two approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Create two subfolders named \"staging\" and \"mart\" within the \"models\" directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Implement the **merge** incremental materialization strategy for inserting and updating the records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.Generate a .sql file named *stg_product__incr* within the ‚Äústaging‚Äù subfolder. Utilize the merge incremental materialization strategy to capture changes of *raw.product* table, add a \"start_date\" arrtbute and output the resulting table to the \"stage\" schema.\n",
    "\n",
    "Quick peek of the initial *stage.stg_product_incr* table:\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/5_2_initial_stg_product.png\" alt=\"5_2_initial_stg_product\" style=\"zoom:100%;\" />\n",
    "\n",
    "Quick peek of the updated *stage.stg_product_incr* table:\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/5_9_stg_product_updated.png\" alt=\"5_9_stg_product_updated\" style=\"zoom:100%;\" />\n",
    "\n",
    "üìù The exact mechanics of how that update/replace takes place will vary depending on your database, incremental strategy, and strategy specific configs.\n",
    "\n",
    "```sql\n",
    "{{\n",
    "    config(\n",
    "        materialized = 'incremental',\n",
    "        incremental_strategy='merge', \n",
    "        unique_key = ['prod_key', 'prod_name', 'vol', 'wgt', 'brand_name', 'status_code', 'status_code_name', 'category_key', 'category_name', 'subcategory_key', 'subcategory_name']\n",
    "    )\n",
    "}}\n",
    "\n",
    "{% if is_incremental() %}\n",
    "\n",
    "    {% set MAX_START_DATE_query %}\n",
    "        select ifnull(max(start_date), '1900-01-01') from {{this}} as MAX_START_DT\n",
    "    {% endset %}\n",
    "\n",
    "    {% if execute %}\n",
    "        {% set MAX_START_DT = run_query(MAX_START_DATE_query).columns[0][0] %}\n",
    "    {% endif %}\n",
    "\n",
    "{% endif %}\n",
    "\n",
    "select \n",
    "    prod_key,\n",
    "    prod_name,\n",
    "    vol,\n",
    "    wgt,\n",
    "    brand_name,\n",
    "    status_code,\n",
    "    status_code_name,\n",
    "    category_key,\n",
    "    category_name,\n",
    "    subcategory_key,\n",
    "    subcategory_name,\n",
    "    sysdate() as start_date\n",
    "from \n",
    "    {{ source('stg', 'product') }}\n",
    "{% if is_incremental() %}\n",
    "    where start_date >= '{{ MAX_START_DT }}'\n",
    "{% endif %}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.Develop a .sql file named *dim_product_incr* within the \"mart\" subfolder. Reference the \"stg_product_incr\" model, use a window function to include a \"deactivate_date\", create a flag named \"active_status\", and output the resulting table to the \"entp\" schema.\n",
    "\n",
    "Quick peek of the initial *entp.dim_product_incr* table:\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/5_4_initial_dim_product__stg.png\" alt=\"5_4_initial_dim_product__stg\" style=\"zoom:100%;\" />\n",
    "\n",
    "Quick peek of the updated *entp.dim_product_incr* table:\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/5_10_dim_product__stg_updated.png\" alt=\"5_10_dim_product__stg_updated\" style=\"zoom:100%;\" />\n",
    "\n",
    "```sql\n",
    "   select \n",
    "      *,\n",
    "      lag(start_date, 1) over(partition by prod_key order by start_date desc) as deactivate_date,\n",
    "      iff(deactivate_date is null, true, false) as active_status\n",
    "   from \n",
    "      {{ ref('stg_product_incr')}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Apply the dbt **snapshot** technique for inserting and updating the records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.Generate a .sql file named *stg_product_snapshot* under the \"snapshots\" directory to capture changes of *raw.product* table and output the resulting table to the \"stage\" schema.\n",
    "\n",
    "Quick peek of the initial *stage.stg_product_snapshot* table:\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/5_6_initial_stg_product_snapshot.png\" alt=\"5_6_initial_stg_product_snapshot\" style=\"zoom:100%;\" />\n",
    "\n",
    "Quick peek of the updated *stage.stg_product_snapshot* table:\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/5_12_stg_product_snapshot_updated.png\" alt=\"5_12_stg_product_snapshot_updated\" style=\"zoom:100%;\" />\n",
    "\n",
    "```sql\n",
    "   {% snapshot stg_product_snapshot %}\n",
    "\n",
    "      {{\n",
    "         config(\n",
    "            target_schema='stage',\n",
    "            strategy='check',\n",
    "            unique_key='prod_key',\n",
    "            check_cols='all',\n",
    "         )\n",
    "      }}\n",
    "\n",
    "      select \n",
    "         * \n",
    "      from \n",
    "         {{ source('stg', 'product') }}\n",
    "      order by\n",
    "         prod_key\n",
    "\n",
    "   {% endsnapshot %}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.Develop a .sql file named *dim_product_snapshot* within the \"mart\" subfolder. Reference the \"stg_product_snapshot\", use \"dbt_valid_from\" and \"dbt_valid_to\" to add \"start_date\", \"deactivate_date\", and a flag named \"active_status\", and output the resulting table to the \"entp\" schema.\n",
    "\n",
    "Quick peek of the initial *entp.dim_product_snapshot* table:\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/5_8_initial_dim_product__snapshot.png\" alt=\"5_8_initial_dim_product__snapshot\" style=\"zoom:100%;\" />\n",
    "\n",
    "Quick peek of the updated *entp.dim_product_snapshot* table:\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/5_13_dim_product__snapshot_updated.png\" alt=\"5_13_dim_product__snapshot_updated\" style=\"zoom:100%;\" />\n",
    "\n",
    "```sql\n",
    "   select \n",
    "      prod_key,\n",
    "      prod_name,\n",
    "      vol,\n",
    "      wgt,\n",
    "      brand_name,\n",
    "      status_code,\n",
    "      status_code_name,\n",
    "      category_key,\n",
    "      category_name,\n",
    "      subcategory_key,\n",
    "      subcategory_name,\n",
    "      dbt_valid_from as start_date,\n",
    "      dbt_valid_to as deactivate_date,\n",
    "      iff(dbt_valid_to is null, true, false) as active_status\n",
    "   from \n",
    "      {{ ref('stg_product_snapshot') }}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales table\n",
    "\n",
    "Quick peek of the initial *raw.sales* table:\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/6_1_initial_sales.png\" alt=\"6_1_initial_sales\" style=\"zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Implement the **delete+insert** incremental materialization strategy for inserting and updating the records.\n",
    "\n",
    "Create a .sql file named *fct_daily_sales* inside the \"mart\" subfolder. Employ the delete+insert incremental materialization strategy to capture changes from the *raw.sales* table, aggregate the sales data to achieve daily granularity, and add an \"update time\" attribute for each record.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/6_3_fct_daily_sales__stg.png\" alt=\"6_3_fct_daily_sales__stg\" style=\"zoom:100%;\" />\n",
    "\n",
    "```sql\n",
    "   {{\n",
    "      config(\n",
    "      materialized = 'incremental',\n",
    "      incremental_strategy='delete+insert',\n",
    "      unique_key = ['cal_dt', 'prod_key', 'store_key']\n",
    "      )\n",
    "   }}\n",
    "\n",
    "   {% if is_incremental() %}\n",
    "\n",
    "      {% set MAX_CAL_DATE_query %}\n",
    "         select ifnull(max(cal_dt), '1900-01-01') from {{this}} as MAX_CAL_DT\n",
    "      {% endset %}\n",
    "\n",
    "      {% if execute %}\n",
    "         {% set MAX_CAL_DT = run_query(MAX_CAL_DATE_query).columns[0][0] %}\n",
    "      {% endif %}\n",
    "\n",
    "   {% endif %}\n",
    "\n",
    "\n",
    "   select\n",
    "      trans_dt as cal_dt,\n",
    "      store_key as store_key,\n",
    "      prod_key as prod_key,\n",
    "      sum(sales_qty) as sales_qty,\n",
    "      sum(sales_amt) as sales_amt,\n",
    "      avg(sales_price) as sales_price,\n",
    "      sum(sales_cost) as sales_cost,\n",
    "      sum(sales_mgrn) as sales_mgrn,\n",
    "      avg(discount) as discount,\n",
    "      sum(ship_cost) as ship_cost,\n",
    "      current_date() as update_time\n",
    "   from \n",
    "      {{ source('stg', 'sales') }}\n",
    "   {% if is_incremental() %}\n",
    "      where trans_dt >= '{{ MAX_CAL_DT }}'\n",
    "   {% endif %}\n",
    "   group by \n",
    "      1,2,3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "\n",
    "Adding a dbt package to your dbt project is an effective way to extend its functionality by leveraging shared models, macros, or even tests from the dbt community or your own custom packages. Here are the steps to add a dbt package to your project:\n",
    "\n",
    "1. Find a Package: Identify the package you wish to include. The [dbt Hub](https://hub.getdbt.com/) is a great resource to find dbt packages created and maintained by the community.\n",
    "\n",
    "2. Update your **packages.yml** File: Every dbt project has a packages.yml file in its root directory. If you don't have one yet, create it. Add the details of the package you want to include.\n",
    "\n",
    "   - For a package hosted on dbt Hub, it might look like:\n",
    "\n",
    "      ```yml\n",
    "         packages:\n",
    "           - package: dbt-labs/dbt_utils\n",
    "             version: 1.1.1\n",
    "      ```\n",
    "   - For a package on GitHub:\n",
    "\n",
    "      ```yml\n",
    "         packages:\n",
    "           - git: \"https://github.com/dbt-labs/dbt-utils.git\"\n",
    "             revision: 0.9.2\n",
    "      ```\n",
    "3. Install the Package: After you've added the desired package(s) to packages.yml, you'll need to install it. Run the following command:\n",
    "   ```bash\n",
    "      dbt deps\n",
    "   ```\n",
    "   This will download and store the package in the dbt_modules directory within your dbt project.\n",
    "\n",
    "4. Use the Package: Now, you can reference and use macros, models, or other resources provided by the package in your dbt project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "> If any test *fails*, dbt will provide detailed information on the failure, helping to ensure that your data transformations are producing accurate and reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column-level Test\n",
    "\n",
    "In dbt, generic tests are a way to test your data models against predefined criteria, ensuring that your transformed data adheres to certain expectations. These are tests that can be applied across many models and columns without needing to be rewritten for specific use cases.\n",
    "\n",
    "Here are some of the most common generic tests provided by dbt: \n",
    "- `unique`: This test checks if values in a given column or set of columns are unique.\n",
    "- `not_null`: Validates that values in a specified column are not null.\n",
    "- `accepted_values`: This tests checks if values in a column are among a set of accepted values.\n",
    "- `relationships`: Validates foreign key relationships between tables. It checks if values in a column of one model also exist in a column of another model.\n",
    "- `expression_is_true`: This test checks whether a given SQL expression evaluates to true for all records in a model.\n",
    "\n",
    "These are just some of the most commonly used generic tests in dbt. The strength of dbt testing is that you can also create custom tests tailored to your specific use cases if the generic tests do not cover your needs.\n",
    "\n",
    "```models.yml\n",
    "   version: 2\n",
    "\n",
    "   models:\n",
    "     - name: <model_name>\n",
    "       columns:\n",
    "         - name: <column_name>\n",
    "           tests:\n",
    "             - ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example \n",
    "\n",
    "Let's say you want to test that the values in product_key column of your product table are unique and does not contain any null values\n",
    "\n",
    "```models.yml\n",
    "   version: 2\n",
    "\n",
    "   models:\n",
    "     - name: dim_product_incr\n",
    "       columns:\n",
    "         - name: prod_key\n",
    "           tests:\n",
    "             - unique\n",
    "             - not_null\n",
    "```\n",
    "\n",
    "Once tests are defined in your schema.yml file, you can run them with the command:\n",
    "```bash\n",
    "   dbt test\n",
    "```\n",
    "\n",
    "### Outputs\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/7_1_dbt_test_unique.png\" alt=\"7_1_dbt_test_unique\" style=\"zoom:100%;\" />\n",
    "\n",
    "\\\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/7_2_dbt_test_not_null.png\" alt=\"7_2_dbt_test_not_null\" style=\"zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-level Test\n",
    "\n",
    "[dbt-expectations](https://github.com/calogica/dbt-expectations) is an extension of dbt's native testing capabilities. Inspired by the \"Great Expectations\" tool, dbt-expectations provides a suite of pre-built, but customizable, data tests that users can implement in their dbt projects. By using dbt-expectations, you can add more comprehensive and varied data quality checks to your transformation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's say you are expecting the model to have grouped rows that are at least as recent as the defined interval prior to the current timestamp. Use [expect_grouped_row_values_to_have_recent_data](https://github.com/calogica/dbt-expectations/blob/main/macros/schema_tests/table_shape/expect_grouped_row_values_to_have_recent_data.sql) to test whether there is recent data for each grouped row defined by group_by (which is a list of columns) and a timestamp_column. Optionally gives the possibility to apply filters on the results.\n",
    "\n",
    "1. First, you need to incorporate dbt-expectations into your project by adding it to your packages.yml:\n",
    "   ```yml\n",
    "      packages:\n",
    "      ...\n",
    "\n",
    "        - package: calogica/dbt_expectations\n",
    "          version: [\">=0.9.0\", \"<0.10.0\"]\n",
    "   ```\n",
    "\n",
    "2. Run `dbt deps` to install the package.\n",
    "\n",
    "3. Before start test, the following variables need to be defined in your dbt_project.yml file:\n",
    "\n",
    "   ```yml\n",
    "      vars:\n",
    "        \"dbt_date:time_zone\": \"America/Los_Angeles\"\n",
    "   ```\n",
    "   You may specify any valid timezone string in place of America/Los_Angeles. For example, use America/New_York for East Coast Time.\n",
    "\n",
    "4. Now, you can use the built-in tests provided by dbt-expectations in your schema.yml:\n",
    "\n",
    "   ```yml\n",
    "      version: 2\n",
    "\n",
    "      models:\n",
    "      ...\n",
    "\n",
    "        - name: fct_daily_sales\n",
    "          tests:\n",
    "            - dbt_expectations.expect_grouped_row_values_to_have_recent_data:\n",
    "                group_by: [store_key]\n",
    "                timestamp_column: cal_dt\n",
    "                datepart: day\n",
    "                interval: 1\n",
    "\n",
    "            - dbt_expectations.expect_grouped_row_values_to_have_recent_data:\n",
    "                group_by: [store_key]\n",
    "                timestamp_column: update_time\n",
    "                datepart: day\n",
    "                interval: 2\n",
    "            \n",
    "   ```\n",
    "\n",
    "5. Once tests are defined in your schema.yml file, you can run them with the command:\n",
    "   ```bash\n",
    "      dbt test\n",
    "   ```\n",
    "\n",
    "### Outputs\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/7_3_dbt_test_cal_dt.png\" alt=\"7_3_dbt_test_cal_dt\" style=\"zoom:100%;\" />\n",
    "\n",
    "\\\n",
    "<img src=\"https://s3.amazonaws.com/weclouddata/images/data_engineer/7_4_dbt_test_update_time.png\" alt=\"7_4_dbt_test_update_time\" style=\"zoom:100%;\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
